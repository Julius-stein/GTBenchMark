out_dir: ./results/test
metric_best: mae
metric_agg: argmin
num_workers: 16

dataset:
  format: PyG-ZINC
  name: subset
  task: graph
  task_type: regression
  transductive: False
  node_encoder: True

  node_encoder_num_types: 28
  edge_encoder_num_types: 4

train:
  mode: custom
  sampler: full_batch
  batch_size: 256
  eval_period: 4
  ckpt_period: 100
  tqdm: True
  persistent_workers: True
  pin_memory: True

val:
  sampler: full_batch

model:
  type: GTModel
  loss_fun: l1
  edge_decoding: dot
  graph_pooling: graph_token

gt:
  layers: 12
  attn_heads: 8
  dim_hidden: 96
  dropout: 0.5
  attn_dropout: 0.5
  input_dropout: 0.1
  activation_dropout: 0.1
  ffn_dim: 96
  node_encoder_list: [TypeDictNode]
  edge_encoder_list: [DummyEdge]
  encoder_type: cascade
  attn_type: LinearAttn
  layer_type: NodeFormerConv
  prepend_norm: True
  batch_norm: False
  layer_norm: True
  residual: True
  act: gelu
  head: reg_graph

  # === NodeFormer-specific ===
  kernel_trans: softmax          # 核函数变换方式
  rb_order: 2                    # Random Feature 阶数
  rb_trans: sigmoid              # Random Feature 变换函数
  nb_random_features: 30         # 采样特征数
  use_edge_loss: False           # 回归任务可设 False
  use_act: True
  use_gumbel: True
  graph_weight: 1.0              # Graph-level loss 权重
  edge_loss_weight: 0.0          # Edge-level loss 权重（可为 0）

gnn:
  mode: "off"
  head: graphormer_graph
  layers_pre_mp: 0
  layers_post_mp: 3
  dim_inner: 80
  batchnorm: True
  act: relu
  dropout: 0.0
  agg: mean
  normalize_adj: False

optim:
  clip_grad_norm: True
  clip_grad_norm_value: 5.0
  optimizer: adamW
  weight_decay: 0.01
  base_lr: 0.001
  max_epoch: 1
  scheduler: polynomial_with_warmup

perf:
  mode: "off"
  with_stack: True
